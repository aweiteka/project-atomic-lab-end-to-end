== geard
geard is a Docker container orchestration tool.  At the currrent release, it essentially does three things:

. SSH
. Multiple container deployment
. Link

=== geard Prerequisites

. Check to see that geard and Docker are installed and running

----
rpm -qa | grep -i geard
rpm -qa | grep -i docker
systemctl status geard
systemctl status docker
----


=== Part 1. Deploy a Single Container
. Check which units geard has registered
+
----
gear list-units
----
. Check to see which images are available and running within Docker
+
----
docker images
docker ps
----
. Install the first unit
+
----
gear install demo/apache web-server --start -p 80:80
----
. List the units again
+
----
gear list-units
----
. Show the container is also recognized by Docker
+
----
docker ps
----
. Make sure the web server is responding to requests
+
----
curl http://localhost
----
. List the units one more time
+
----
gear list-units
----
. Clean up the environmentnd
+
----
gear delete web-server
gear list-units
----


=== Part 2. Deploy Multiple Containers on a Single Host
. Check the geard environment
+
----
gear list-units
docker ps
----
. Check out the _http_single_HTB.json_ file 
+
----
cat http_single_HTB.json
{
  "Containers": [
    {
      "Name": "web-server",
      "Image": "demo/apache",
      "PublicPorts": [
        {
          "Internal": 80
        }
      ],
      "Links": [
        {
          "To": "web-server",
          "NonLocal": true,
          "MatchPort": true
        }
      ],
      "Count": 3
    }
  ],
  "IdPrefix": "",
  "RandomizeIds": false
}
----
This file tells geard to deploy an application with _Name_ "web-server", to use _image_ "demo/apache" with an internal port of 80 and a count of 3.

. Check out the _/var/lib/containers/_ directory.  This is where geard stores it's content. Right now there will not be any content in the _units_ directory.  After a successful launch of the application there will be.
+
----
ls /var/lib/containers/
ls /var/lib/containers/units/
----
. Deploy the application. This tells geard to deploy all three web servers on the same host.
+
----
gear deploy http_single_HTB.json
----
. Check the _/var/lib/containers/_ directory again to see what content geard added
+
----
ls /var/lib/containers/
ls /var/lib/containers/units/
----
. 













. Create a network in Admin -> Networks 
.. Use the name _public_ or _external_ or something similar
.. Select _admin_ for project
.. Make it an External Network
.. For Allocation Pool: 172.16.0.100,172.16.0.200
. Create a new project by going to Admin -> Projects -> Create Project









+
NOTE: If you get an error check the horizon log at _/var/log/horizon/horizon.log_ most likey this is a bug and can be fixed by editing _/etc/openstack-dashboard/local_settings_ and change the line +OPENSTACK_KEYSTONE_DEFAULT_ROLE = "Member"+ to +OPENSTACK_KEYSTONE_DEFAULT_ROLE = "\_member_"+ and restart +httpd+. However, puppet will overwrite this change later but that will not affect the creation of the project right now.
+
. Create a user in Admin -> Users and add it to the newly created project
+
NOTE: You may get an error that the user could not be added to the project but if you check it worked.
+
. Logout as admin
. Login as the new user
+
NOTE: *The remainder of these steps will be done as this user*
+
. Create a keypair by going to Projects -> Access & Security -> Keypairs
. Download the private key and set permissions on it
+
----
chmod 600 <downloaded_user_key>.pem
----
+
. Upload an image by going to Project -> Images & Snapshots
. Upload the CirrOS imagelocation http://download.cirros-cloud.net/0.3.1/cirros-0.3.1-x86_64-disk.img or see the CLI section below to upload a RHEL image
. Create a network in Project -> Networks 
.. Call this private or similar
.. Under Subnet tab provide a name such as private
.. Network Address: make up a new private network such as 192.168.0.0/24
.. Leave the rest of the values as default
.. Leave DHCP enabled
.. Leave Allocation Pools blank
.. DNS Name Servers: 172.16.0.2
. Fill out the subnet details
. Create a router at Project -> Routers
.. On the right side of the screen on the newly created router select _Set Gateway_ 
.. For _External Network_ select the external network created by admin
.. Click on the router name
.. Select _Add Interface_ and select the private subnet
. Launch an instance at Project -> Instances
.. For flavor use m1.small for the rhel-guest-image-6 or m1.tiny for cirros
.. For _Instance Boot Source_ select _Boot from Image_
.. Select the desired image
.. For _Access & Security_ the key you created earlier should be selected
.. For Networking select the private network
+
NOTE: This instance shouly only see the private network
+
. Allocate a floating IP at Project -> Access & Security -> Floating IPs -> Allocate IPs to Project
. Associate the floating IP with the instance at Project -> Instances -> More
. Click on the instane name and view the log or console
+
NOTE: It make take a few minutes for the log or console to show up
+
. SSH to your instance and enjoy
+
----
ssh -i key.pem cloud-user@floating_ip
----

=== Validate from the CLI
One of the first steps in configuring OpenStack is to upload an image and boot an instance. A small image is available called _cirros_ or a small RHEL 6.5 image can be used.

. First load the keystone variables so the CLI commands can authenticate
+
----
source /root/keystonerc_admin
----
+
. Setup an SSH keypair
+
----
nova keypair-add testkey > /root/testkey.pem
chmod 600 /root/testkey.pem
----

==== Images

Using the CirrOS image

. If your controller can access the internet directly, then load the cirros image:
+
----
glance image-create --name "cirros-0.3.1-x86_64" --disk-format qcow2 --container-format bare --location http://download.cirros-cloud.net/0.3.1/cirros-0.3.1-x86_64-disk.img --is-public true
----
+
NOTE: You can also download the image directly on your laptop and scp it to the controller and load it with +--file+ instead.
+
. List images
+
----
glance image-list
----
+
. Show the details of the cirros image and confirm the size is not 0:
+
----
glance image-show cirros-0.3.1-x86_64
----

Using the RHEL 6.5 image:

. Install the RHEL OpenStack guest image:
+
----
yum install rhel-guest-image-6
----
+
----
glance image-create --name rhel-6 --disk-format qcow2 --container-format bare --file /usr/share/rhel-guest-image-6/rhel-guest-image-6-6.5-20140121.0-1.qcow2 --is-public true
----
+
. List images
+
----
glance image-list
----
+
. Show the details of the cirros image and confirm the size is not 0:
+
----
glance image-show rhel-6
----

==== Networking

===== Nova Network

Update the default security group to allow ping and SSH

----
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/24
nova secgroup-add-rule default tcp 22 22 0.0.0.0/24
----

Launch an Instance
----
nova flavor-list
nova image-list
nova boot testserver --flavor 2 --image cirros-0.3.1-x86_64 --key-name testkey --security-groups default
----

Add a floating IP
----
# In Nova Network
nova-manage floating list 
# Delete any already used management IP addresses!
nova-manage floating delete 172.16.0.1
nova-manage floating delete 172.16.0.10
nova-manage floating delete 172.16.0.11
nova-manage floating delete 172.16.0.12

nova add-floating-ip testserver 172.16.0.2
----

Proceed to <<Connect_To_Instance>>

===== Neutron Network

. Create security group rules to allow +icmp+ and +ssh+ traffic
+
----
neutron security-group-rule-create --protocol icmp --direction ingress default
neutron security-group-rule-create --protocol tcp --port-range-min 22 --port-range-max 22 --direction ingress default
----
+
. Create an external network and subnet as admin
+
----
neutron net-create pubnet1 --router:external=True
neutron subnet-create --name pubsubnet1 --dns-nameserver 172.16.0.1 --allocation-pool start=172.16.0.100,end=172.16.0.150 --disable-dhcp pubnet1 172.16.0.0/24
----
+
. Capture IDs
+
----
PUBNETID=$(neutron net-list | grep pubnet1 | awk '{print $2}')
----
+
. Create Internal private network
+
----
neutron net-create privnet1
neutron subnet-create --name privsubnet1 privnet1 10.0.0.0/24
----
+
. Capture Subnet UUID
+
----
PRIVSUBNET1ID=$(neutron subnet-list | grep privsubnet1 | awk '{print $2}')
----
+
. Create router for L3 to connect each network
+
----
neutron router-create router1
neutron router-interface-add router1 $PRIVSUBNET1ID
----
+
. Set the gateway for your external (physical) network
+
----
neutron router-gateway-set router1 $PUBNETID
----

Launch an Instance

. List available flavors
+
----
nova flavor-list
----
+
. List images
+
----
nova image-list
----
+
. List networks
+
----
neutron net-list
----
+
. Boot an instance
+
----
nova boot testserver --flavor 2 --image cirros-0.3.1-x86_64 --key-name testkey --security-groups default --nic net-id=<NET ID>
----
+
. List instances
+
----
nova list
----
+
NOTE: Wait for an _ACTIVE_ status
+
. View the console logs
+
----
nova console-log testserver
----
+
. Add a floating IP. First get the instance ID
+
----
nova list
instid=$(nova list | awk '/instance01 / {print $2}')
----
+
. Grab the port ID
+
----
portid=$(neutron port-list --device_id ${instid} | awk '/ip_address/ {print $2}') 
----
+
. Copy the tenant keystone file to the neutron server
+
----
scp /root/keystonerc_${USERNAME} ${NEUTRON_NODE}:/root/.
scp /root/${USERNAME}.pem ${NEUTRON_NODE}:/root/.
----
+
. Grab the router ID of the tenant from the Networker node. This could be done on the controller but adds an extra check to make sure neutron server is working
+
----
routerid=$(ssh ${NEUTRON_NODE} "source /root/keystonerc_${USERNAME} &&  neutron router-list" | awk '/router1/ {print $2}')
----
+
. Grab the qrouter that matches the router ID
+
----
qrouterid=$(ssh ${NEUTRON_NODE} "source /root/keystonerc_${USERNAME} && ip netns list | grep $routerid")
----
+
. Grab the private IP and ping it from the neutron server over netns 
+
----
privateip=$(ssh ${NEUTRON_NODE} "source /root/keystonerc_${USERNAME} && neutron port-list --device_id ${instid}" | awk '/ip_address/ {print $10}' |  awk -F'"' '{print $2}')
----
+
. Ping internal IP over netns
+
----
ssh ${NEUTRON_NODE} "source /root/keystonerc_${USERNAME} && ip netns exec $qrouterid ping -c 3 $privateip"
----
+
. Assign a floating IP to the port
+
----
neutron floatingip-create --port-id $portid ${EXT_NET}
----
+
. Grab the floating ip
+
----
floatip=$( neutron floatingip-list | awk "/$privateip / { print \$6 }")
----
+
. Run +nova list+ until the floating ip shows up
. Ping floating IP over netns
+
----
ssh ${NEUTRON_NODE} "source /root/keystonerc_${USERNAME} && ip netns exec $qrouterid ping -c 3 $floatip"
----
+
. Ping the floating IP
+
----
ping -c 3 $floatip
----

Proceed to <<Connect_To_Instance>>

[[Connect_To_Instance]]
==== Connect to Instance

Confirm you can connect to the instance through the floating IP
----
ssh -i /root/testkey.pem cirros@172.16.0.2
ssh -i /root/testkey.pem cloud-user@172.16.0.2
----
